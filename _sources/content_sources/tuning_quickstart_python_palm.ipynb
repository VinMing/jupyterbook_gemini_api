{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaLM API：使用 Python 微调快速入门\n",
    "在本笔记本中，您将了解如何使用 PaLM API 的 Python 客户端库开始使用调整服务。在这里，您将学习如何调整 PaLM API 文本生成服务背后的文本模型。\n",
    "```{tip}\n",
    "注意：目前，调整仅适用于 text-bison-001 模型。\n",
    "```\n",
    "\n",
    "## 安装\n",
    "### 认证\n",
    "PaLM API 允许您根据自己的数据调整模型。由于这是您的数据和调整后的模型，因此需要比 API 密钥提供的更严格的访问控制。\n",
    "\n",
    "在运行本教程之前，您需要[为您的项目设置 OAuth](https://ai.google.dev/palm_docs/oauth_quickstart)。\n",
    "\n",
    "如果您想在 Colab 中运行此笔记本，请首先使用“文件 > 上传”选项上传`client_secret*.json`文件。\n",
    "```shell\n",
    "cp client_secret*.json client_secret.json\n",
    "ls client_secret.json\n",
    "```\n",
    "此 gcloud 命令将 client_secret.json 文件转换为可用于对服务进行身份验证的凭据。\n",
    "\n",
    "```{importance}\n",
    "重要提示：如果您在 Colab 中运行此程序，请不要仅单击它打印的链接。那会失败的。按照说明操作，将其打印的 gcloud 命令复制到本地计算机并在那里运行，然后将本地计算机的输出粘贴回此处。\n",
    "```\n",
    "\n",
    "```python\n",
    "import os\n",
    "if 'COLAB_RELEASE_TAG' in os.environ:\n",
    "  # Use `--no-browser` in colab\n",
    "  !gcloud auth application-default login --no-browser --client-id-file client_secret.json --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.tuning'\n",
    "else:\n",
    "  !gcloud auth application-default login --client-id-file client_secret.json --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.tuning'\n",
    "\n",
    "```\n",
    "\n",
    "### 安装客户端库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入客户端库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"YOUR-API-KEY\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/chat-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 Chat (Legacy)',\n",
      "      description='A legacy text-only model optimized for chat conversations',\n",
      "      input_token_limit=4096,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateMessage', 'countMessageTokens'],\n",
      "      temperature=0.25,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/text-bison-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='PaLM 2 (Legacy)',\n",
      "      description='A legacy model that understands text and generates text as an output',\n",
      "      input_token_limit=8196,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateText', 'countTextTokens', 'createTunedTextModel'],\n",
      "      temperature=0.7,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n",
      "Model(name='models/embedding-gecko-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding Gecko',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=1024,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedText', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/gemini-pro',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Pro',\n",
      "      description='The best model for scaling across a wide range of tasks',\n",
      "      input_token_limit=30720,\n",
      "      output_token_limit=2048,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.9,\n",
      "      top_p=1.0,\n",
      "      top_k=1)\n",
      "Model(name='models/gemini-pro-vision',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini Pro Vision',\n",
      "      description='The best image understanding model to handle a broad range of applications',\n",
      "      input_token_limit=12288,\n",
      "      output_token_limit=4096,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=0.4,\n",
      "      top_p=1.0,\n",
      "      top_k=32)\n",
      "Model(name='models/embedding-001',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Embedding 001',\n",
      "      description='Obtain a distributed representation of a text.',\n",
      "      input_token_limit=2048,\n",
      "      output_token_limit=1,\n",
      "      supported_generation_methods=['embedContent', 'countTextTokens'],\n",
      "      temperature=None,\n",
      "      top_p=None,\n",
      "      top_k=None)\n",
      "Model(name='models/aqa',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Model that performs Attributed Question Answering.',\n",
      "      description=('Model trained to return answers to questions that are grounded in provided '\n",
      "                   'sources, along with estimating answerable probability.'),\n",
      "      input_token_limit=7168,\n",
      "      output_token_limit=1024,\n",
      "      supported_generation_methods=['generateAnswer'],\n",
      "      temperature=0.2,\n",
      "      top_p=1.0,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "for model in genai.list_models():\n",
    "    pprint.pprint(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以使用 genai.list_tuned_model 方法检查现有的调整模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, m in zip(range(5), genai.list_tuned_models()):\n",
    "  print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建调整模型\n",
    "要创建调整模型，您需要将数据集传递给`genai.create_tuned_model`方法中的模型。您可以通过直接定义调用中的输入和输出值或从文件导入数据帧以传递给方法来执行此操作。\n",
    "\n",
    "对于此示例，您将调整模型以生成序列中的下一个数字。例如，如果输入为 1，则模型应输出 2。如果输入为 100，则输出应为 101。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/text-bison-001'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = [\n",
    "    m for m in genai.list_models()\n",
    "    if \"createTunedTextModel\" in m.supported_generation_methods][0]\n",
    "base_model.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "name = f'generate-num-{random.randint(0,10000)}'\n",
    "operation = genai.create_tuned_model(\n",
    "    # You can use a tuned model here too. Set `source_model=\"tunedModels/...\"`\n",
    "    source_model=base_model.name,\n",
    "    training_data=[\n",
    "        {\n",
    "             'text_input': '1',\n",
    "             'output': '2',\n",
    "        },{\n",
    "             'text_input': '3',\n",
    "             'output': '4',\n",
    "        },{\n",
    "             'text_input': '-3',\n",
    "             'output': '-2',\n",
    "        },{\n",
    "             'text_input': 'twenty two',\n",
    "             'output': 'twenty three',\n",
    "        },{\n",
    "             'text_input': 'two hundred',\n",
    "             'output': 'two hundred one',\n",
    "        },{\n",
    "             'text_input': 'ninety nine',\n",
    "             'output': 'one hundred',\n",
    "        },{\n",
    "             'text_input': '8',\n",
    "             'output': '9',\n",
    "        },{\n",
    "             'text_input': '-98',\n",
    "             'output': '-97',\n",
    "        },{\n",
    "             'text_input': '1,000',\n",
    "             'output': '1,001',\n",
    "        },{\n",
    "             'text_input': '10,100,000',\n",
    "             'output': '10,100,001',\n",
    "        },{\n",
    "             'text_input': 'thirteen',\n",
    "             'output': 'fourteen',\n",
    "        },{\n",
    "             'text_input': 'eighty',\n",
    "             'output': 'eighty one',\n",
    "        },{\n",
    "             'text_input': 'one',\n",
    "             'output': 'two',\n",
    "        },{\n",
    "             'text_input': 'three',\n",
    "             'output': 'four',\n",
    "        },{\n",
    "             'text_input': 'seven',\n",
    "             'output': 'eight',\n",
    "        }\n",
    "    ],\n",
    "    id = name,\n",
    "    epoch_count = 100,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.001,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您的调整后的模型会立即添加到调整后的模型列表中，但在模型调整时其状态会设置为“正在创建”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.get_tuned_model(f'tunedModels/{name}')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检查调整进度\n",
    "使用`元数据`检查状态："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operation.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`operation.result()`或`operation.wait_bar()`等待训练完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "for status in operation.wait_bar():\n",
    "  time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以随时使用 cancel() 方法取消调整作业。取消注释下面的行并运行代码单元以在作业完成之前取消作业。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# operation.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调优完成后，您可以从调优结果中查看损耗曲线。[损失曲线](https://generativeai.devsite.corp.google.com/guide/model_tuning_guidance#recommended_configurations)显示模型的预测与理想输出的偏差程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "model = operation.result()\n",
    "\n",
    "snapshots = pd.DataFrame(model.tuning_task.snapshots)\n",
    "\n",
    "sns.lineplot(data=snapshots, x = 'epoch', y='mean_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估您的模型\n",
    "您可以使用`genai.generate_text`方法并指定模型名称来测试模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = genai.generate_text(model=f'tunedModels/{name}',\n",
    "                                prompt='5')\n",
    "completion.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = genai.generate_text(model=f'tunedModels/{name}',\n",
    "                                prompt='-9')\n",
    "completion.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = genai.generate_text(model=f'tunedModels/{name}',\n",
    "                                prompt='four')\n",
    "completion.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如您所看到的，最后一个提示没有产生理想的结果，即`5`。为了产生更好的结果，您可以尝试一些不同的方法，例如将温度调整为接近零以获得更一致的结果，向数据集中添加更多质量示例以供模型学习，或者向示例添加提示或序言。\n",
    "\n",
    "有关提高性能的更多指导，请参阅[调优指南](https://generativeai.devsite.corp.google.com/guide/model_tuning_guidance)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更新描述\n",
    "您可以随时使用`genai.update_tuned_model`方法更新调整模型的描述。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.update_tuned_model(f'tunedModels/{name}', {\"description\":\"This is my model.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.get_tuned_model(f'tunedModels/{name}')\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 删除模型\n",
    "您可以通过删除不再需要的模型来清理调整后的模型列表。使用`genai.delete_tuned_model`方法删除模型。如果您取消了任何调整作业，您可能需要删除这些作业，因为它们的性能可能无法预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.delete_tuned_model(f'tunedModels/{name}')\n",
    "\n",
    "try:\n",
    "  m = genai.get_tuned_model(f'tunedModels/{name}')\n",
    "  print(m)\n",
    "except Exception as e:\n",
    "  print(f\"{type(e)}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nb (gemini)",
   "language": "python",
   "name": "gemini"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
